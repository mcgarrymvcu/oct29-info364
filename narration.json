{
  "01": "A data warehouse’s logical database design is the blueprint that organizes how information will be structured and accessed before any physical implementation. It focuses on defining fact tables, dimension tables, relationships, and hierarchies that support analytical queries. Unlike operational databases, the logical design emphasizes integration, consistency, and ease of reporting across large volumes of historical data. In this lecture, we’ll examine how to translate business requirements into a logical model that balances performance, scalability, and clarity",
  "02": "A data warehouse provides a single, trusted source of truth by consolidating information from multiple systems into one centralized repository. This enables faster, more accurate decision-making since managers and analysts can work with consistent, historical data instead of scattered reports. It also supports trend analysis and forecasting, giving organizations a competitive edge by identifying opportunities and risks earlier. Ultimately, a data warehouse improves efficiency, reduces duplication, and enhances the strategic use of data across the business.",
  "03": "Transactional databases are built for speed on individual records \u2014 things like processing a sale or updating a customer\u2019s address. Analytical systems, on the other hand, are built to answer big-picture questions, like sales trends over time or inventory levels by region. The problem is that if you try to run analytics on a transactional system, it slows down daily operations. That\u2019s why organizations separate the two: OLTP for operations, OLAP for decision-making.",
  "04": "OLTP (Online Transaction Processing) systems are designed for handling day-to-day business operations like order entry, payments, or customer updates—optimized for fast inserts, updates, and queries on small amounts of data. OLAP (Online Analytical Processing), on the other hand, is built for analyzing large volumes of historical data, enabling complex queries, aggregations, and trend analysis across multiple dimensions. In short, OLTP powers real-time transactions, while OLAP supports strategic decision-making through data analysis and reporting.",
  "05": "When we talk about deploying a data warehouse, we’re really talking about a structured, step-by-step methodology. It begins with defining business requirements so we’re clear on what questions the warehouse should answer. From there, we move into data modeling and ETL—extracting, transforming, and loading data from multiple sources into a consistent structure. The next steps involve building the warehouse itself, testing for data quality, and then deploying tools like OLAP or dashboards for analysis. The overall goal is to deliver a reliable system that turns raw data into useful insights for decision-making.",
  "06": "This slide shows a slightly different perspective on how to build a data warehouse, starting with identifying the key business entities—like products, regions, or time periods—and the measures we want to analyze, such as revenue or sales. Next, we determine the grain of the fact tables, meaning the level of detail for each record, which drives both the accuracy and the size of the data. From there, we design a star schema to connect facts and dimensions, making sure aggregations across dimensions don’t create errors with things like averages or ratios. Finally, we tie the design back to actual data sources, define the ETL processes, and load the data so it’s ready for reporting and analysis.",
  "07": "A fact table holds the measurable, numeric data that represents business events, like sales amounts, quantities ordered, or revenue. You can think of it as the scoreboard that captures what happened. A dimension table provides the descriptive context that explains those facts, such as product names, customer details, time periods, or store locations. When you put them together, the fact table tells you the numbers, and the dimension tables let you slice and analyze those numbers by different perspectives, like sales by product or revenue by region.",
  "08": "A star schema is a way of organizing data in a warehouse where a central fact table is linked to multiple dimension tables. The fact table holds the numbers we want to analyze, while the dimension tables provide descriptive details that give those numbers meaning. It’s called a star schema because the diagram looks like a star, with the fact table in the center and dimensions radiating outward. This design is widely used in data warehousing because it’s simple, easy to understand, and makes queries run faster by minimizing the number of joins and making aggregations more efficient, which is especially important for large analytical workloads.",
  "09": "In a star schema, data is organized into a central fact table with surrounding dimension tables, making it easy to query and optimized for analysis. The focus is on simplicity and speed, so data is often denormalized, meaning there may be some repetition in the dimension tables. In contrast, a third normal form (3NF) model follows strict normalization rules to reduce redundancy, splitting data into many related tables. This makes 3NF efficient for transaction processing systems but more complex and slower for analytical queries, since joining across many tables takes more time. In practice, star schemas are preferred for data warehouses because they prioritize fast, straightforward analysis, while 3NF is better suited for operational databases that handle frequent updates and inserts.",
  "10": "To create a star schema, you start by identifying the business process you want to analyze, such as sales or inventory. Next, define the fact table by choosing the measurable events or metrics, like revenue, quantity, or cost. Then, identify the dimensions that provide context for those facts, such as product, customer, time, or location. Once defined, design the tables so that the fact table sits in the center with foreign keys linking to each dimension table. The dimension tables should be denormalized to include descriptive attributes that make analysis easier, like product category or customer region. Finally, connect the schema to source data through ETL processes so it can be populated and used for reporting and analysis.",
  "11": "In this slide, the star schema is shown through a sales example. At the center is the sales fact table, which contains the numeric measures like revenue or quantity sold. Surrounding it are the dimension tables—such as customer, product, time, and region—that provide descriptive details to analyze those sales. The design looks like a star, with the fact table in the middle and dimensions radiating outward. This setup makes analysis straightforward and efficient, since queries only need to join the fact table with the relevant dimensions.\n\nFor example, a query might ask: “Show total sales revenue by product category and month.” In the star schema, this simply means joining the sales fact table with the product and time dimensions, then grouping by category and month to get the results.",
  "12": "A primary key is a unique identifier for each record in a table, ensuring that no two rows are exactly the same. In a dimension table, this might be something like a product ID or a customer ID, which links back to the fact table. Sometimes, however, real-world identifiers are messy, inconsistent, or can change over time, which is where surrogate keys come in. A surrogate key is an artificially created value, often just an auto-incrementing number, that serves as a stable and simple primary key. Using surrogate keys makes it easier to manage slowly changing dimensions and maintain clean relationships across tables. In data warehouses, it’s common practice to rely on surrogate keys for dimension tables, while the fact table references them as foreign keys to ensure consistency.",
  "13": "",
  "14": "",
  "15": "",
  "16": "",
  "17": "",
  "18": "",
  "19": "",
  "20": "",
  "21": "",
  "22": "",
  "23": "",
  "24": "",
  "25": "",
  "25": "",
  "27": "",
  "28": ""
}
