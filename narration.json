{
  "01": "A data warehouse’s logical database design is the blueprint that organizes how information will be structured and accessed before any physical implementation. It focuses on defining fact tables, dimension tables, relationships, and hierarchies that support analytical queries. Unlike operational databases, the logical design emphasizes integration, consistency, and ease of reporting across large volumes of historical data. In this lecture, we’ll examine how to translate business requirements into a logical model that balances performance, scalability, and clarity",
  "02": "A data warehouse provides a single, trusted source of truth by consolidating information from multiple systems into one centralized repository. This enables faster, more accurate decision-making since managers and analysts can work with consistent, historical data instead of scattered reports. It also supports trend analysis and forecasting, giving organizations a competitive edge by identifying opportunities and risks earlier. Ultimately, a data warehouse improves efficiency, reduces duplication, and enhances the strategic use of data across the business.",
  "03": "Transactional databases are built for speed on individual records \u2014 things like processing a sale or updating a customer\u2019s address. Analytical systems, on the other hand, are built to answer big-picture questions, like sales trends over time or inventory levels by region. The problem is that if you try to run analytics on a transactional system, it slows down daily operations. That\u2019s why organizations separate the two: OLTP for operations, OLAP for decision-making.",
  "04": "OLTP (Online Transaction Processing) systems are designed for handling day-to-day business operations like order entry, payments, or customer updates—optimized for fast inserts, updates, and queries on small amounts of data. OLAP (Online Analytical Processing), on the other hand, is built for analyzing large volumes of historical data, enabling complex queries, aggregations, and trend analysis across multiple dimensions. In short, OLTP powers real-time transactions, while OLAP supports strategic decision-making through data analysis and reporting.",
  "05": "When we talk about deploying a data warehouse, we’re really talking about a structured, step-by-step methodology. It begins with defining business requirements so we’re clear on what questions the warehouse should answer. From there, we move into data modeling and ETL—extracting, transforming, and loading data from multiple sources into a consistent structure. The next steps involve building the warehouse itself, testing for data quality, and then deploying tools like OLAP or dashboards for analysis. The overall goal is to deliver a reliable system that turns raw data into useful insights for decision-making.",
  "06": "This slide shows a slightly different perspective on how to build a data warehouse, starting with identifying the key business entities—like products, regions, or time periods—and the measures we want to analyze, such as revenue or sales. Next, we determine the grain of the fact tables, meaning the level of detail for each record, which drives both the accuracy and the size of the data. From there, we design a star schema to connect facts and dimensions, making sure aggregations across dimensions don’t create errors with things like averages or ratios. Finally, we tie the design back to actual data sources, define the ETL processes, and load the data so it’s ready for reporting and analysis.",
  "07": "A fact table holds the measurable, numeric data that represents business events, like sales amounts, quantities ordered, or revenue. You can think of it as the scoreboard that captures what happened. A dimension table provides the descriptive context that explains those facts, such as product names, customer details, time periods, or store locations. When you put them together, the fact table tells you the numbers, and the dimension tables let you slice and analyze those numbers by different perspectives, like sales by product or revenue by region.",
  "08": "A star schema is a way of organizing data in a warehouse where a central fact table is linked to multiple dimension tables. The fact table holds the numbers we want to analyze, while the dimension tables provide descriptive details that give those numbers meaning. It’s called a star schema because the diagram looks like a star, with the fact table in the center and dimensions radiating outward. This design is widely used in data warehousing because it’s simple, easy to understand, and makes queries run faster by minimizing the number of joins and making aggregations more efficient, which is especially important for large analytical workloads.",
  "09": "In a star schema, data is organized into a central fact table with surrounding dimension tables, making it easy to query and optimized for analysis. The focus is on simplicity and speed, so data is often denormalized, meaning there may be some repetition in the dimension tables. In contrast, a third normal form (3NF) model follows strict normalization rules to reduce redundancy, splitting data into many related tables. This makes 3NF efficient for transaction processing systems but more complex and slower for analytical queries, since joining across many tables takes more time. In practice, star schemas are preferred for data warehouses because they prioritize fast, straightforward analysis, while 3NF is better suited for operational databases that handle frequent updates and inserts.",
  "10": "To create a star schema, you start by identifying the business process you want to analyze, such as sales or inventory. Next, define the fact table by choosing the measurable events or metrics, like revenue, quantity, or cost. Then, identify the dimensions that provide context for those facts, such as product, customer, time, or location. Once defined, design the tables so that the fact table sits in the center with foreign keys linking to each dimension table. The dimension tables should be denormalized to include descriptive attributes that make analysis easier, like product category or customer region. Finally, connect the schema to source data through ETL processes so it can be populated and used for reporting and analysis.",
  "11": "In this slide, the star schema is shown through a sales example. At the center is the sales fact table, which contains the numeric measures like revenue or quantity sold. Surrounding it are the dimension tables—such as customer, product, time, and region—that provide descriptive details to analyze those sales. The design looks like a star, with the fact table in the middle and dimensions radiating outward. This setup makes analysis straightforward and efficient, since queries only need to join the fact table with the relevant dimensions.\n\nFor example, a query might ask: “Show total sales revenue by product category and month.” In the star schema, this simply means joining the sales fact table with the product and time dimensions, then grouping by category and month to get the results.",
  "12": "A primary key is a unique identifier for each record in a table, ensuring that no two rows are exactly the same. In a dimension table, this might be something like a product ID or a customer ID, which links back to the fact table. Sometimes, however, real-world identifiers are messy, inconsistent, or can change over time, which is where surrogate keys come in. A surrogate key is an artificially created value, often just an auto-incrementing number, that serves as a stable and simple primary key. Using surrogate keys makes it easier to manage slowly changing dimensions and maintain clean relationships across tables. In data warehouses, it’s common practice to rely on surrogate keys for dimension tables, while the fact table references them as foreign keys to ensure consistency.",
  "13": "In a data warehouse, the grain defines the level of detail captured in a fact table, such as transactions per day, per order, or per line item. Choosing the right grain is one of the most important design decisions, because it sets the foundation for how flexible and useful the warehouse will be. A grain that is too detailed can lead to massive data volumes and performance challenges, while a grain that is too coarse may prevent analysts from answering key business questions. The right approach is to align the grain with how the business actually measures performance—for example, if profitability is tracked at the order line level, that’s where the grain should be set. A good rule of thumb is to choose the lowest level of detail that the business regularly analyzes, since aggregation is always possible later, but drilling down into missing detail is not. In practice, this requires conversations with stakeholders to balance business needs, system performance, and storage considerations.",
  "14": "Aggregation problems in data warehousing occur when summarized data produces misleading or inconsistent results because of how dimensions and hierarchies are structured. For example, if a product belongs to multiple categories, rolling up sales by category may double-count revenue unless relationships are carefully defined. Similarly, incomplete hierarchies can make drill-down analysis difficult, because data may appear to disappear or fail to roll up correctly. These issues often stem from non-strict relationships in dimensions, where entities can belong to multiple parents. To manage this, designers need to enforce clear hierarchies, validate dimension data, and sometimes introduce bridge tables or carefully designed surrogate keys. At a practical level, understanding these pitfalls helps analysts trust the results of roll-up and drill-down operations, which are central to decision-making.",
  "15": "Aggregation problems arise when data is rolled up across dimensions in ways that create ambiguity or distortion. For example, if a customer belongs to multiple regions or categories, simply summing sales across those dimensions may double-count results. This issue exists because dimension relationships in a warehouse are not always strict one-to-one; they can be many-to-many or incomplete. As a result, drill-down or roll-up operations can produce misleading totals that don’t match reality. The core challenge is ensuring that measures are aggregated at the right grain and with clear, consistent relationships. Without careful design, these problems undermine the trustworthiness of reports and dashboards, even when the raw data itself is accurate.",
  "16": "Aggregation problems occur because dimensions don’t always have clean, one-to-one relationships with facts, which can lead to double counting or incomplete totals. The solution is to carefully design hierarchies and relationships in your dimension tables so that drill-down and roll-up operations follow clear, consistent rules. One approach is to enforce strict hierarchies (e.g., each store belongs to one and only one region) so aggregations don’t overlap. When that’s not possible, you may need to define bridge tables or many-to-many relationship tables that control how facts roll up. Another key strategy is choosing the right grain for your fact table, which ensures that measures are stored at the lowest meaningful level and aggregated upward correctly. In practice, solving these problems often means working closely with business users to clarify which totals are meaningful and which are misleading.\n\nExample (Bridge Table Solution: Suppose you are tracking student enrollment and each student can take multiple courses. If you just total students across all courses, you risk double counting anyone enrolled in more than one class. To fix this, you create a bridge table that maps each student to each course. The fact table then records enrollment at the “student–course” level rather than just at the student level. When you aggregate using the bridge table, you get accurate counts of enrollments per course while still being able to calculate unique student totals without inflation.",
  "17": "This data mart is designed to support reporting on financial performance at different levels of detail. For example, it can generate income statements by period, showing revenues and expenses rolled up by account. It also supports trend analysis, such as comparing revenue growth across months or years. Managers could use it to create department-level budget reports, tracking whether spending is aligned with planned allocations. In addition, the data mart allows for drill-down reports that move from overall financial performance into specific accounts or transactions. By organizing the data around facts and dimensions, the mart ensures consistency in reporting and enables both high-level executive summaries and detailed operational view.",
  "18": "To convert a 3NF order processing design into a star schema, the first step is to identify the business process being measured—in this case, customer orders. The central fact table will store measures such as order quantity, sales amount, and discount, at the chosen grain (e.g., order line item). Next, the normalized reference tables—like customer, product, employee, and time—are transformed into dimension tables with denormalized, descriptive attributes. For example, instead of multiple normalized tables for product category and supplier, the product dimension will contain all relevant descriptive fields in one place. Relationships are simplified to direct links between each dimension and the fact table, eliminating the chains of joins common in 3NF. This structure makes querying much faster and easier, because analysts can directly slice and dice orders by product, customer, region, or time. The end result is a star schema that balances storage efficiency with accessibility, making it well-suited for reporting and analytics.",
  "19": "In this design, sales are captured at the order line level, which provides the most flexible and detailed grain for analysis. The fact table records the measures of interest, such as sales amount or quantity, while dimensions like Product, Shop, and Customer provide the context for slicing and dicing the data. By linking these dimensions directly to the fact table, analysts can easily run reports that show sales by product category, by individual store, or by specific customer segments. This avoids the complexity of multiple joins required in a normalized design and ensures queries run more efficiently. The star schema also makes it easier for business users to understand and navigate the data model.",
  "20": "To build a star schema for this employee transaction system, the fact table should focus on the measurable business process—salaries. The fact table would store salary amounts, pay periods, and related metrics at the employee–time level. Dimension tables such as Employee, Department, Job Role, and Time provide descriptive attributes to analyze compensation from multiple perspectives. For example, the Employee dimension can include demographics and hire date, while the Department and Job Role dimensions allow comparison across organizational structures. The Time dimension enables tracking of salary trends by month, quarter, or year. This design makes it straightforward to answer questions like salary by department, average pay by job role, or salary growth over time without the complexity of a normalized system.",
  "21": "In this employee star schema, the fact table captures salary amounts at the employee–period level, which provides the right grain for consistent reporting. Dimensions such as Employee, Department, Job Role, and Time give context to the measures, allowing analysis across organizational units, roles, or time frames. By flattening these dimensions, users can easily answer questions like “average salary by department” or “salary growth by quarter” without navigating complex joins. This structure also supports historical tracking, since slowly changing dimensions can preserve past job roles or departmental assignments. The star schema design ensures reporting is both accurate and fast, while making the model intuitive for business users to explore.",
  "22": "The subject area for this data mart is focused on analyzing student enrollment. The fact table should capture enrollment events at the student–course–term level, with measures such as credit hours, tuition paid, or grade earned. Key dimension tables would include Student (demographics, status), Course (subject, level, department), Instructor, and Time (term, semester, year). This structure allows users to explore questions like how many students enrolled in a given course, which departments have the highest credit hour production, or how enrollment trends shift across semesters. By modeling the enrollment process in a star schema, the data mart supports both high-level summaries (e.g., total enrollments by college) and detailed drill-downs (e.g., student performance in specific sections).\n\nExample reports from this data mart could include enrollment by major, showing how student interest changes across programs, or tuition revenue by semester, providing insight into financial performance over time. Another useful report would be average class size by department, which helps administrators allocate teaching resources more effectively.",
  "23": "When dimension values change over time, a data warehouse needs a strategy to manage those updates. A Type 1 change simply overwrites the old value, which is simple but erases history. For example, if a customer’s name is misspelled and later corrected, Type 1 just replaces the incorrect value without keeping the old version. A Type 2 change creates a new row with start and end dates, preserving history but adding more rows and complexity—this is best when tracking changes like addresses or job titles over time. A Type 3 change keeps both current and previous values in separate columns, offering limited history and is often used when only the most recent change is relevant, such as current versus prior account manager. The choice depends on whether accuracy, history, or simplicity matters most for the business.",
  "24": "A Type 2 change preserves history by creating a new row in the dimension table whenever an attribute value changes. Each row includes start and end dates (or an active flag) so the warehouse can track which values were valid at different points in time. For example, if a person’s tax bracket changes from medium to high, the old row remains in the table with an end date, and a new row is added with the updated bracket and a new start date. This approach allows analysts to run historical reports accurately, reflecting the values that were true at the time. The tradeoff is that it increases the size of the dimension table and can make queries more complex, but it is the most reliable method when complete history is required. Because fact tables reference the surrogate keys from the dimension table, they automatically link to the correct version of the dimension at the time the transaction occurred.",
  "25": "A Type 3 change tracks limited history by adding extra columns to the dimension table for both the current and previous values. Unlike Type 2, it does not create new rows; instead, it keeps just the most recent change available for analysis. For example, if a person’s tax bracket changes from medium to high, the table would show the current value as “high” and retain a column with the prior value as “medium.” This approach is useful when only the last change matters, such as comparing current and prior states, but it does not preserve a full history of all changes. The advantage is simplicity, but the tradeoff is that you lose visibility into older changes beyond the most recent one./n/nIn summary, Type 1 changes overwrite values with no history, Type 2 changes add new rows to preserve full history, and Type 3 changes store only the current and prior values, giving limited history with less complexity.",
  "25": "A snowflake schema is a variation of the star schema where dimension tables are normalized into multiple related tables. This design can reduce storage costs and improve update performance when dimensions are very large or frequently changing. However, the tradeoff is added complexity—queries must join across more tables, which slows performance and makes reporting harder to manage. For example, instead of a single “Customer” dimension, you might have separate tables for customer, region, and country. While snowflake schemas were more appealing when storage was expensive, today storage is cheap and query performance is more important. For that reason, snowflake schemas have fallen out of favor in most modern data warehouse designs.\n\nA star schema prioritizes simplicity and query speed with denormalized dimensions, while a snowflake schema normalizes dimensions to save space but sacrifices performance and ease of use.",
  "27": "n this lecture, we covered the fundamentals of designing a data warehouse. We began by contrasting transactional systems (OLTP) with analytical systems (OLAP), emphasizing why organizations separate the two. We introduced dimensional modeling and the star schema as the standard design, focusing on fact tables, dimension tables, surrogate keys, and the importance of choosing the right grain. We also explored summarizability problems—like incomplete drill-downs, roll-ups, and non-strict relationships—and how to resolve them. Next, we worked through examples of converting operational data into a star schema and discussed how slowly changing dimensions can be handled using Type 1, Type 2, and Type 3 approaches. Finally, we looked at the snowflake schema, noting that while it normalizes dimension tables to reduce storage, it is largely out of favor today due to slower queries and added complexity.\n\nPlease complete the data warehousing design exercise listed on the last slide. This assignment reinforces the design principles we discussed, including identifying facts and dimensions, setting the appropriate grain, and constructing a usable schema. It will also help prepare you for applying these concepts to more complex scenarios later in the course.\n\nIn our next class, we will turn to ETL (Extract, Transform, Load). ETL is the process that brings raw data from operational systems into the warehouse, cleans and reshapes it, and loads it for analysis. We will focus on data quality, transformation logic, and how ETL pipelines ensure consistent, repeatable reporting."
}
